% Written by Anders Sjoqvist and Ulf Lundstrom, 2009
% The main sources are: tinyKACTL, Beta and Wikipedia

\chapter{Mathematics}

\subsection{Game Theory}

Sprague-Grundy theorem hypothesis: the game is impartial, each player have the same set of moves, a player that can't move loses.
To calculate the SG number:
\begin{itemize}
    \item The SG number of the sum two games with numbers $SG_1$ and $SG_2$ is $SG_1 \oplus SG_2$
    \item The SG number of a game is $\mathrm{mex} \; S$ where $S$ is the set of SG numbers of the games you can move to, and $\mathrm{mex} \; S = \mathrm{min} \; \mathbb{N} \setminus S$
    \item The first player has a winning strategy $\Leftrightarrow$ the SG number of the game is $\neq 0$
\end{itemize}
Classical Games (\ding{182} last one wins (normal); \ding{183} last one loses (mis\`ere))\\

\underline{\textbf{NIM}}: \\
$n$ piles of objs. One can take any number of objs from any pile (i.e. set of possible moves for the $i$-th pile is $M=[pile_i]$, $[x]:=\{1,2,...,\lfloor x \rfloor\}$).\\
$SG=\otimes_{i=1}^n pile_i$. Strategy: \ding{182} make the Nim-Sum 0 by \emph{decreasing} a heap; \ding{183} the same, except when the normal move would only leave heaps of size 1. In that case, leave an odd number of 1's.
The result of \ding{183} is the same as \ding{182}, opposite if all piles are 1's. Many games are essentially NIM.\\
\underline{\textbf{NIM (powers)}}\\
$M = \{a^m|m\ge 0\}$\\
If $a$ odd: $SG_n = n \% 2$\\
If $a$ even: $SG_n = 2$, if $n\equiv a\%(a+1)$; $SG_n = n \% (a+1) \% 2$, else.\\
\underline{\textbf{NIM (half)}}\\
$M_{\text{\ding{172}}} = [\frac{pile_i}{2}]$ \newline $M_{\text{\ding{173}}} = [\lceil \frac{pile_i}{2} \rceil\text{, } pile_i]$\\
\ding{172}$SG_{2n} = n$, $SG_{2n+1}=SG_{n}$ \newline \ding{173}$SG_0=0$, $SG_n=[\log_2 n]+1$
\\ 

\underline{\textbf{NIM (divisors)}}\\
$M_{\text{\ding{172}}} = \text{divisors of } pile_i$ \newline $M_{\text{\ding{173}}} = \text{proper divisors of }pile_i$\\
\ding{172}$SG_0 = 0$, $SG_n = SG_{\text{\ding{173}},n} + 1$ \newline \ding{173}$SG_1=0$, $SG_n=$ number of 0's at the end of $n_{binary}$\\

\underline{\textbf{Subtraction Game}}\\
$M_{\text{\ding{172}}} = [k]$ $M_{\text{\ding{173}}} = S$ (finite) $M_{\text{\ding{174}}} = S \cup \{pile_i\}$\\
$SG_{\text{\ding{172}},n} = n \mod (k+1)$. \ding{182}lose if $SG=0$; \ding{183}lose if $SG=1$. $SG_{\text{\ding{174}},n} = SG_{\text{\ding{173}},n}+1$\\
For any finite $M$, $SG$ of one pile is eventually periodic.
\\

\underline{\textbf{Moore's NIM$_k$}}\\
One can take any number of objs from at most k piles.\\
\ding{182}Write $pile_i$ in base $k+1$, sum up each digit without carry. Losing if the result is 0.\\
\ding{183} If all piles are 1's, losing iff $n\equiv 1\%(k+1)$. Otherwise the result is the same as \ding{182}.
\\ 

\underline{\textbf{Staircase NIM}}\\
	$n$ piles in a line. One can take any number of objs from $pile_i$, $i>0$ to $pile_{i-1}$\\
	Losing if the NIM formed by the odd-indexed piles is losing(i.e. $\otimes_{i=0}^{(n-1)/2} pile_{2i+1}=0$)
	\\ 
	
	\underline{\textbf{Lasker's NIM}}\\
	Two possible moves: 1.take any number of objs; 2.split a pile into two (no obj removed) \\
	$SG_n = n, \text{ if }n\equiv 1,2(\% 4)$ $SG_n = n+1, \text{ if }n\equiv3(\% 4)$ $SG_n = n-1, \text{ if }n\equiv0(\% 4)$
	\\ 
	
	\underline{\textbf{Kayles}} \\
	Two possible moves: 1.take 1 or 2 objs; 2.split a pile into two (after removing objs) \\
	$SG_n$ for small $n$ can be computed recursively. $SG_n$ for $n \in [72,83]$: 4 1 2 8 1 4 7 2 1 8 2 7
	$SG_n$ becomes periodic from the 72-th item with period length 12.
	\\ 
	
	\underline{\textbf{Dawson's Chess}} \\
	$n$ boxes in a line. One can occupy a box if its neighbours are not occupied. \\
	$SG_n$ for $n\in [1,18]$: 1 1 2 0 3 1 1 0 3 3 2 2 4 0 5 2 2 3
	Period = 34 from the 52-th item.
	\\ 
	
	\underline{\textbf{Wythoff's Game}}\\
	 \textbf{Two} piles of objs. One can take any number of objs from either pile, or take the \emph{same} number from \emph{both} piles. \\
	$n_k = \lfloor k \phi \rfloor = \lfloor m_k \phi \rfloor -m_k$  $m_k = \lfloor k \phi^2 \rfloor = \lceil n_k \phi \rceil = n_k + k$  $\phi:=\frac{1+\sqrt{5}}{2}$. $(n_k,m_k)$ is the $k$-th losing position.\\
	$n_k$ and $m_k$ form a pair of complementary Beatty Sequences (since$\frac{1}{\phi}+\frac{1}{\phi^2}=1$). Every $x>0$ appears either in $n_k$ or in $m_k$.
	\\
	
	\underline{\textbf{Mock Turtles}}\\
	$n$ coins in a line. One can turn over 1, 2 or 3 coins, with the rightmost from head to tail. \\
	$SG_n = 2n$, if $\mathrm{ones}(2n)$ odd; $SG_n = 2n + 1$, else. $\mathrm{ones}(x)$: the number of 1's in $x_{binary}$\\
	$SG_n$ for $n\in [0,10]$ (leftmost position is 0): 1 2 4 7 8 11 13 14 16 19 21
	\\ 
	
	\underline{\textbf{Ruler}} \\
	$n$ coins in a line. One can turn over any \emph{consecutive} coins, with the rightmost from head to tail. \\
	$SG_n=$ the largest power of 2 dividing $n$. This is implemented as $n$\&$-n$(lowbit)\\
	$SG_n$ for $n\in [1,10]$: 1 2 1 4 1 2 1 8 1 2
	\\ 
	
	\underline{\textbf{Divisors}} \\
	You have a number $n$. One can divide it at each turn. \\
	$SG_n=$ number of prime factors of $n$
	\\ 

\section{Equations}
In general, given an equation $Ax = b$, the solution to a variable $x_i$ is given by
\[x_i = \frac{\det A_i'}{\det A} \]
where $A_i'$ is $A$ with the $i$'th column replaced by $b$.

\section{Trigonometry}
\begin{align*}
\sin(v+w)&{}=\sin v\cos w+\cos v\sin w\\
\cos(v+w)&{}=\cos v\cos w-\sin v\sin w\\
\end{align*}
\begin{align*}
\tan(v+w)&{}=\dfrac{\tan v+\tan w}{1-\tan v\tan w}\\
\sin v+\sin w&{}=2\sin\dfrac{v+w}{2}\cos\dfrac{v-w}{2}\\
\cos v+\cos w&{}=2\cos\dfrac{v+w}{2}\cos\dfrac{v-w}{2}
\end{align*}
\[ (V+W)\tan(v-w)/2{}=(V-W)\tan(v+w)/2 \]
where $V, W$ are lengths of sides opposite angles $v, w$.
\begin{align*}
	a\cos x+b\sin x&=r\cos(x-\phi)\\
	a\sin x+b\cos x&=r\sin(x+\phi)
\end{align*}
where $r=\sqrt{a^2+b^2}, \phi=\operatorname{atan2}(b,a)$.

\section{Geometry}

\subsection{Triangles}
Side lengths: $a,b,c$\\
Semiperimeter: $p=\dfrac{a+b+c}{2}$\\
Area: $A=\sqrt{p(p-a)(p-b)(p-c)}$\\
Circumradius: $R=\dfrac{abc}{4A}$\\
Inradius: $r=\dfrac{A}{p}$\\
Length of median (divides triangle into two equal-area triangles): $m_a=\tfrac{1}{2}\sqrt{2b^2+2c^2-a^2}$\\
Length of bisector (divides angles in two): $s_a=\sqrt{bc\left[1-\left(\dfrac{a}{b+c}\right)^2\right]}$\\
Law of sines: $\dfrac{\sin\alpha}{a}=\dfrac{\sin\beta}{b}=\dfrac{\sin\gamma}{c}=\dfrac{1}{2R}$\\
Law of cosines: $a^2=b^2+c^2-2bc\cos\alpha$\\
Law of tangents: $\dfrac{a+b}{a-b}=\dfrac{\tan\dfrac{\alpha+\beta}{2}}{\tan\dfrac{\alpha-\beta}{2}}$ \bigskip

\subsection{Quadrilaterals}
With side lengths $a,b,c,d$, diagonals $e, f$, diagonals angle $\theta$, area $A$ and
magic flux $F=b^2+d^2-a^2-c^2$:

\[ 4A = 2ef \cdot \sin\theta = F\tan\theta = \sqrt{4e^2f^2-F^2} \]

 For cyclic quadrilaterals the sum of opposite angles is $180^\circ$,
$ef = ac + bd$, and $A = \sqrt{(p-a)(p-b)(p-c)(p-d)}$.
\bigskip

\subsection{Spherical coordinates}
\begin{center}
\includegraphics[width=25mm]{content/math/sphericalCoordinates}
\end{center}
\[\begin{array}{cc}
x = r\sin\theta\cos\phi & r = \sqrt{x^2+y^2+z^2}\\
y = r\sin\theta\sin\phi & \theta = \textrm{acos}(z/\sqrt{x^2+y^2+z^2})\\
z = r\cos\theta & \phi = \textrm{atan2}(y,x)
\end{array}\]
\subsection{Formulas}
For a planar graph, $v - e + f = 2$\\
Pick Theorem : $S = I + B/2-1$. Where $S$ is area, $B$ number of points on boundary and $I$ points strictly inside. 

\section{Derivatives/Integrals}
\begin{align*}
	\dfrac{d}{dx}\arcsin x = \dfrac{1}{\sqrt{1-x^2}} &&& \dfrac{d}{dx}\arccos x = -\dfrac{1}{\sqrt{1-x^2}} \\
	\dfrac{d}{dx}\tan x = 1+\tan^2 x &&& \dfrac{d}{dx}\arctan x = \dfrac{1}{1+x^2} \\
	\int\tan ax = -\dfrac{\ln|\cos ax|}{a} &&& \int x\sin ax = \dfrac{\sin ax-ax \cos ax}{a^2} \\
	\int e^{-x^2} = \frac{\sqrt \pi}{2} \text{erf}(x) &&& \int xe^{ax}dx = \frac{e^{ax}}{a^2}(ax-1)
\end{align*}

Integration by parts:
\[\int_a^bf(x)g(x)dx = [F(x)g(x)]_a^b-\int_a^bF(x)g'(x)dx\]

\section{Sums}
\begin{align*}
	1^4 + 2^4 + 3^4 + \dots + n^4 &= \frac{n(n+1)(2n+1)(3n^2 + 3n - 1)}{30} \\
\end{align*}
$$\sqrt{1+x} = 1+\frac{x}{2}-\frac{x^2}{8}+\frac{2x^3}{32}-\frac{5x^4}{128}+\dots,\,(-1\leq x\leq1)$$

\section{Probability theory}
%Let $X$ be a discrete random variable with probability $p_X(x)$ of assuming the value $x$. It will then have an expected value (mean) $\mu=\mathbb{E}(X)=\sum_xxp_X(x)$ and variance $\sigma^2=V(X)=\mathbb{E}(X^2)-(\mathbb{E}(X))^2=\sum_x(x-\mathbb{E}(X))^2p_X(x)$ where $\sigma$ is the standard deviation. If $X$ is instead continuous it will have a probability density function $f_X(x)$ and the sums above will instead be integrals with $p_X(x)$ replaced by $f_X(x)$.

%Expectation is linear:
%\[\mathbb{E}(aX+bY) = a\mathbb{E}(X)+b\mathbb{E}(Y)\]
%For independent $X$ and $Y$, \[V(aX+bY) = a^2V(X)+b^2V(Y).\]
\subsection{Discrete distributions}

\subsection{Continuous distributions}

\subsubsection{Uniform distribution}
If the probability density function is constant between $a$ and $b$ and 0 elsewhere it is $\textrm{U}(a,b),\,a<b$.
\[f(x) = \left\{
\begin{array}{cl}
\frac{1}{b-a} & a<x<b\\
0 & \textrm{otherwise}
\end{array}\right.\]
\[\mu=\frac{a+b}{2},\,\sigma^2=\frac{(b-a)^2}{12}\]

\subsubsection{Exponential distribution}
The time between events in a Poisson process is $\textrm{Exp}(\lambda),\,\lambda>0$.
\[f(x) = \left\{
\begin{array}{cl}
\lambda e^{-\lambda x} & x\geq0\\
0 & x<0
\end{array}\right.\]
\[\mu=\frac{1}{\lambda},\,\sigma^2=\frac{1}{\lambda^2}\]

\subsubsection{Normal distribution}
Most real random values with mean $\mu$ and variance $\sigma^2$ are well described by $\mathcal{N}(\mu,\sigma^2),\,\sigma>0$.
\[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]
If $X_1 \sim \mathcal{N}(\mu_1,\sigma_1^2)$ and $X_2 \sim \mathcal{N}(\mu_2,\sigma_2^2)$ then
\[ aX_1 + bX_2 + c \sim \mathcal{N}(\mu_1+\mu_2+c,a^2\sigma_1^2+b^2\sigma_2^2) \]

\section{Markov chains}
A \emph{Markov chain} is a discrete random process with the property that the next state depends only on the current state.
Let $X_1,X_2,\ldots$ be a sequence of random variables generated by the Markov process.
Then there is a transition matrix $\mathbf{P} = (p_{ij})$, with $p_{ij} = \Pr(X_n = i | X_{n-1} = j)$,
and $\mathbf{p}^{(n)} = \mathbf P^n \mathbf p^{(0)}$ is the probability distribution for $X_n$ (i.e., $p^{(n)}_i = \Pr(X_n = i)$),
where $\mathbf{p}^{(0)}$ is the initial distribution.

% \subsubsection{Stationary distribution}
$\mathbf{\pi}$ is a stationary distribution if $\mathbf{\pi} = \mathbf{\pi P}$.
If the Markov chain is \emph{irreducible} (it is possible to get to any state from any state),
then $\pi_i = \frac{1}{\mathbb{E}(T_i)}$ where $\mathbb{E}(T_i)$  is the expected time between two visits in state $i$.
$\pi_j/\pi_i$ is the expected number of visits in state $j$ between two visits in state $i$.

For a connected, undirected and non-bipartite graph, where the transition probability is uniform among all neighbors, $\pi_i$ is proportional to node $i$'s degree.

% \subsubsection{Ergodicity}
A Markov chain is \emph{ergodic} if the asymptotic distribution is independent of the initial distribution.
A finite Markov chain is ergodic iff it is irreducible and \emph{aperiodic} (i.e., the gcd of cycle lengths is 1).
$\lim_{k\rightarrow\infty}\mathbf{P}^k = \mathbf{1}\pi$.

% \subsubsection{Absorption}
A Markov chain is an A-chain if the states can be partitioned into two sets $\mathbf{A}$ and $\mathbf{G}$, such that all states in $\mathbf{A}$ are absorbing ($p_{ii}=1$), and all states in $\mathbf{G}$ leads to an absorbing state in $\mathbf{A}$.
The probability for absorption in state $i\in\mathbf{A}$, when the initial state is $j$, is $a_{ij} = p_{ij}+\sum_{k\in\mathbf{G}} a_{ik}p_{kj}$.
The expected time until absorption, when the initial state is $i$, is $t_i = 1+\sum_{k\in\mathbf{G}}p_{ki}t_k$.

